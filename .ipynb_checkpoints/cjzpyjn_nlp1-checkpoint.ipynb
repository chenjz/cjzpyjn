{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Practice 1: computing text similarities using gensim and nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25-03-2016 18:42:01 - cjzpy_load_logging - load_logging_json - INFO - Loading my universal log service: cjzpy_logging.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully imported ColorizingStreamHandler from logutils\n",
      "Failed importing ColorizingStreamHandler from logutils, using logging.StreamHandler()\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import gensim\n",
    "import sys, logging\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "# all output goes to logs \n",
    "sys.path.append('~/logs')\n",
    "import cjzpy_load_logging\n",
    "cjzpy_load_logging.load_logging_json(default_level=logging.INFO)\n",
    "logger = logging.getLogger(sys._getframe().f_code.co_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logger.info('hello NLP practice 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gensim4texts(logger, texts, query, nTopic):\n",
    "    logger.info('This is %s', sys._getframe().f_code.co_name)\n",
    "    logger.debug('texts: %s', texts)\n",
    "    # In BoW representation, each document is represented by one vector where each vector element represents a question-answer pair, in the style of: How many times does the word system appear in the document? The mapping between the questions and ids is called a dictionary\n",
    "    dictionary = gensim.corpora.Dictionary(texts)\n",
    "    logger.debug('dictionary.token2id: %s', dictionary.token2id)\n",
    "    # To actually convert tokenized documents to vectors\n",
    "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "    # gensim.corpora.MmCorpus.serialize('corpus_tmp.mm', corpus) # store to disk, for later use\n",
    "    logger.debug('corpus: %s', corpus)\n",
    "    tfidf = gensim.models.TfidfModel(corpus)\n",
    "    corpus_tfidf = tfidf[corpus]\n",
    "    for doc in corpus_tfidf:\n",
    "        logger.debug('corpus_tfidf.doc: %s', doc)\n",
    "    logger.debug('tfidf.dfs: %s', tfidf.dfs)\n",
    "    logger.debug('tfidf.idfs: %s', tfidf.idfs)\n",
    "    lsi = gensim.models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=nTopic)\n",
    "    lsi.print_topics(2)\n",
    "    corpus_lsi = lsi[corpus_tfidf]\n",
    "    for doc in corpus_lsi:\n",
    "        logger.debug('copus_lsi.doc: %s', doc)\n",
    "\n",
    "    lda = gensim.models.LdaModel(corpus_tfidf, id2word=dictionary, num_topics=nTopic)\n",
    "    lda.print_topics(2)\n",
    "    index = gensim.similarities.MatrixSimilarity(lsi[corpus])\n",
    "    #query_bow = dictionary.doc2bow(query.lower().split())\n",
    "    query_bow = dictionary.doc2bow(query)\n",
    "    logger.debug('query_bow: %s', query_bow)\n",
    "    query_lsi = lsi[query_bow]\n",
    "    logger.debug('query_lsi: %s', query_lsi)\n",
    "    sims = index[query_lsi]\n",
    "    logger.debug('sims: %s', list(enumerate(sims)))\n",
    "\n",
    "    sort_sims = sorted(enumerate(sims), key=lambda item: -item[1])\n",
    "    logger.info('sort_sims: %s', sort_sims)\n",
    "    return sort_sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25-03-2016 18:42:10 - <ipython-input-62-a79ba582be37> - <module> - INFO - This is gensim4texts\n",
      "25-03-2016 18:42:10 - dictionary - gensim.corpora.dictionary - INFO - adding document #0 to Dictionary(0 unique tokens: [])\n",
      "25-03-2016 18:42:10 - dictionary - gensim.corpora.dictionary - INFO - built Dictionary(11 unique tokens: ['delivery', 'a', 'silver', 'shipment', 'damaged']...) from 3 documents (total 22 corpus positions)\n",
      "25-03-2016 18:42:10 - tfidfmodel - gensim.models.tfidfmodel - INFO - collecting document frequencies\n",
      "25-03-2016 18:42:10 - tfidfmodel - gensim.models.tfidfmodel - INFO - PROGRESS: processing document #0\n",
      "25-03-2016 18:42:10 - tfidfmodel - gensim.models.tfidfmodel - INFO - calculating IDF weights for 3 documents and 10 features (21 matrix non-zeros)\n",
      "25-03-2016 18:42:10 - lsimodel - gensim.models.lsimodel - INFO - using serial LSI version on this node\n",
      "25-03-2016 18:42:10 - lsimodel - gensim.models.lsimodel - INFO - updating model with new documents\n",
      "25-03-2016 18:42:10 - lsimodel - gensim.models.lsimodel - INFO - preparing a new chunk of documents\n",
      "25-03-2016 18:42:10 - lsimodel - gensim.models.lsimodel - INFO - using 100 extra samples and 2 power iterations\n",
      "25-03-2016 18:42:10 - lsimodel - gensim.models.lsimodel - INFO - 1st phase: constructing (11, 102) action matrix\n",
      "25-03-2016 18:42:10 - lsimodel - gensim.models.lsimodel - INFO - orthonormalizing (11, 102) action matrix\n",
      "25-03-2016 18:42:10 - lsimodel - gensim.models.lsimodel - INFO - 2nd phase: running dense svd on (11, 3) matrix\n",
      "25-03-2016 18:42:10 - lsimodel - gensim.models.lsimodel - INFO - computing the final decomposition\n",
      "25-03-2016 18:42:10 - lsimodel - gensim.models.lsimodel - INFO - keeping 2 factors (discarding 23.571% of energy spectrum)\n",
      "25-03-2016 18:42:10 - lsimodel - gensim.models.lsimodel - INFO - processed documents up to #3\n",
      "25-03-2016 18:42:10 - lsimodel - gensim.models.lsimodel - INFO - topic #0(1.137): 0.438*\"gold\" + 0.438*\"shipment\" + 0.366*\"truck\" + 0.366*\"arrived\" + 0.345*\"fire\" + 0.345*\"damaged\" + 0.297*\"silver\" + 0.149*\"delivery\" + 0.000*\"a\" + -0.000*\"of\"\n",
      "25-03-2016 18:42:10 - lsimodel - gensim.models.lsimodel - INFO - topic #1(1.000): 0.728*\"silver\" + 0.364*\"delivery\" + -0.364*\"damaged\" + -0.364*\"fire\" + 0.134*\"truck\" + 0.134*\"arrived\" + -0.134*\"gold\" + -0.134*\"shipment\" + 0.000*\"of\" + -0.000*\"a\"\n",
      "25-03-2016 18:42:10 - lsimodel - gensim.models.lsimodel - INFO - topic #0(1.137): 0.438*\"gold\" + 0.438*\"shipment\" + 0.366*\"truck\" + 0.366*\"arrived\" + 0.345*\"fire\" + 0.345*\"damaged\" + 0.297*\"silver\" + 0.149*\"delivery\" + 0.000*\"a\" + -0.000*\"of\"\n",
      "25-03-2016 18:42:10 - lsimodel - gensim.models.lsimodel - INFO - topic #1(1.000): 0.728*\"silver\" + 0.364*\"delivery\" + -0.364*\"damaged\" + -0.364*\"fire\" + 0.134*\"truck\" + 0.134*\"arrived\" + -0.134*\"gold\" + -0.134*\"shipment\" + 0.000*\"of\" + -0.000*\"a\"\n",
      "25-03-2016 18:42:10 - ldamodel - gensim.models.ldamodel - INFO - using symmetric alpha at 0.5\n",
      "25-03-2016 18:42:10 - ldamodel - gensim.models.ldamodel - INFO - using symmetric eta at 0.5\n",
      "25-03-2016 18:42:10 - ldamodel - gensim.models.ldamodel - INFO - using serial LDA version on this node\n",
      "25-03-2016 18:42:10 - ldamodel - gensim.models.ldamodel - INFO - running online LDA training, 2 topics, 1 passes over the supplied corpus of 3 documents, updating model once every 3 documents, evaluating perplexity every 3 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "25-03-2016 18:42:10 - ldamodel - gensim.models.ldamodel - WARNING - too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
      "25-03-2016 18:42:10 - ldamodel - gensim.models.ldamodel - INFO - -5.395 per-word bound, 42.1 perplexity estimate based on a held-out corpus of 3 documents with 5 words\n",
      "25-03-2016 18:42:10 - ldamodel - gensim.models.ldamodel - INFO - PROGRESS: pass 0, at document #3/3\n",
      "25-03-2016 18:42:10 - ldamodel - gensim.models.ldamodel - INFO - topic #0 (0.500): 0.120*gold + 0.114*shipment + 0.108*silver + 0.107*fire + 0.104*truck + 0.103*damaged + 0.100*arrived + 0.081*delivery + 0.054*of + 0.054*a\n",
      "25-03-2016 18:42:10 - ldamodel - gensim.models.ldamodel - INFO - topic #1 (0.500): 0.121*silver + 0.102*arrived + 0.099*damaged + 0.097*truck + 0.096*shipment + 0.096*delivery + 0.094*fire + 0.088*gold + 0.069*of + 0.069*a\n",
      "25-03-2016 18:42:10 - ldamodel - gensim.models.ldamodel - INFO - topic diff=0.416839, rho=1.000000\n",
      "25-03-2016 18:42:10 - ldamodel - gensim.models.ldamodel - INFO - topic #0 (0.500): 0.120*gold + 0.114*shipment + 0.108*silver + 0.107*fire + 0.104*truck + 0.103*damaged + 0.100*arrived + 0.081*delivery + 0.054*of + 0.054*a\n",
      "25-03-2016 18:42:10 - ldamodel - gensim.models.ldamodel - INFO - topic #1 (0.500): 0.121*silver + 0.102*arrived + 0.099*damaged + 0.097*truck + 0.096*shipment + 0.096*delivery + 0.094*fire + 0.088*gold + 0.069*of + 0.069*a\n",
      "25-03-2016 18:42:10 - docsim - gensim.similarities.docsim - WARNING - scanning corpus to determine the number of features (consider setting `num_features` explicitly)\n",
      "25-03-2016 18:42:10 - docsim - gensim.similarities.docsim - INFO - creating matrix with 3 documents and 2 features\n",
      "25-03-2016 18:42:10 - <ipython-input-62-a79ba582be37> - <module> - INFO - sort_sims: [(1, 0.93163693), (2, 0.83416492), (0, 0.40757114)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(1, 0.93163693), (2, 0.83416492), (0, 0.40757114)]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = [\"Shipment of gold damaged in a fire\", \"Delivery of silver arrived in a silver truck\", \n",
    " \"Shipment of gold arrived in a truck\"]\n",
    "texts = [[word for word in document.lower().split()] for document in documents]\n",
    "query = 'gold silver truck'\n",
    "gensim4texts(logger, texts, query.lower().split(), 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nltkPreProcess4texts(logger, texts, stemmer=None):\n",
    "# logger: logging object\n",
    "# texts: list of texts for preprocessing with NLTK\n",
    "# stemmer: default is LancasterStemmer; PorterStemmer\n",
    "    logger.info('This is %s', sys._getframe().f_code.co_name)\n",
    "    logger.info('texts=%s', texts[0])\n",
    "# lowering case\n",
    "    texts_lower = [[word for word in document.lower().split()] for document in texts]\n",
    "    logger.debug('texts_lower[0]=%s', texts_lower[0])\n",
    "# tockenizing\n",
    "    texts_tokenized = [[word.lower() for word in nltk.word_tokenize(document)] for document in texts]\n",
    "    logger.debug('texts_tokenized[0]=%s', texts_tokenized[0])\n",
    "# filtering stopwords and punctuations\n",
    "    english_stopwords = nltk.corpus.stopwords.words('english')\n",
    "    english_stopwords.extend([',', '.', ':', ';', '?', '(', ')', '[', ']', '&', '!', '*', '@', '#', '$', '%'])\n",
    "    logger.debug('english_stopwords=%s, length=%d', english_stopwords,len(english_stopwords))\n",
    "    texts_filtered_stopwords = [[word for word in document if not word in english_stopwords] for document in texts_tokenized]\n",
    "    logger.debug('texts_filtered_stopwords[0]=%s', texts_filtered_stopwords[0])\n",
    "# stemmering with LancasterStemmer\n",
    "    if stemmer is 'porter':\n",
    "        st = nltk.PorterStemmer()\n",
    "#        st = nltk.stem.porter.PorterStemmer()\n",
    "    else:\n",
    "        st = nltk.LancasterStemmer()\n",
    "#        st = nltk.stem.lancaster.LancasterStemmer()\n",
    "    texts_stemmed = [[st.stem(word) for word in docment] for docment in texts_filtered_stopwords]\n",
    "    logger.debug('texts_stemmed[0]=%s', texts_stemmed[0])\n",
    "# eliminating words with only one occurence\n",
    "    all_stems = sum(texts_stemmed, [])\n",
    "    stems_once = set(stem for stem in set(all_stems) if all_stems.count(stem) == 1)\n",
    "    texts = [[stem for stem in text if stem not in stems_once] for text in texts_stemmed]\n",
    "    logger.debug('texts_WithMoreThanOneCounts[0]=%s', texts[0])\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25-03-2016 18:37:20 - <ipython-input-43-58ec3b96fc61> - <module> - INFO - This is gensim4texts\n",
      "25-03-2016 18:37:20 - dictionary - gensim.corpora.dictionary - INFO - adding document #0 to Dictionary(0 unique tokens: [])\n",
      "25-03-2016 18:37:20 - dictionary - gensim.corpora.dictionary - INFO - built Dictionary(18 unique tokens: ['的', '的哥', '的士', 'C++', '和']...) from 4 documents (total 22 corpus positions)\n",
      "25-03-2016 18:37:20 - tfidfmodel - gensim.models.tfidfmodel - INFO - collecting document frequencies\n",
      "25-03-2016 18:37:20 - tfidfmodel - gensim.models.tfidfmodel - INFO - PROGRESS: processing document #0\n",
      "25-03-2016 18:37:20 - tfidfmodel - gensim.models.tfidfmodel - INFO - calculating IDF weights for 4 documents and 17 features (22 matrix non-zeros)\n",
      "25-03-2016 18:37:20 - lsimodel - gensim.models.lsimodel - INFO - using serial LSI version on this node\n",
      "25-03-2016 18:37:20 - lsimodel - gensim.models.lsimodel - INFO - updating model with new documents\n",
      "25-03-2016 18:37:20 - lsimodel - gensim.models.lsimodel - INFO - preparing a new chunk of documents\n",
      "25-03-2016 18:37:21 - lsimodel - gensim.models.lsimodel - INFO - using 100 extra samples and 2 power iterations\n",
      "25-03-2016 18:37:21 - lsimodel - gensim.models.lsimodel - INFO - 1st phase: constructing (18, 102) action matrix\n",
      "25-03-2016 18:37:21 - lsimodel - gensim.models.lsimodel - INFO - orthonormalizing (18, 102) action matrix\n",
      "25-03-2016 18:37:21 - lsimodel - gensim.models.lsimodel - INFO - 2nd phase: running dense svd on (18, 4) matrix\n",
      "25-03-2016 18:37:21 - lsimodel - gensim.models.lsimodel - INFO - computing the final decomposition\n",
      "25-03-2016 18:37:21 - lsimodel - gensim.models.lsimodel - INFO - keeping 2 factors (discarding 44.074% of energy spectrum)\n",
      "25-03-2016 18:37:21 - lsimodel - gensim.models.lsimodel - INFO - processed documents up to #4\n",
      "25-03-2016 18:37:21 - lsimodel - gensim.models.lsimodel - INFO - topic #0(1.077): 0.383*\"的士\" + 0.344*\"黑色\" + 0.306*\"开\" + 0.306*\"他\" + 0.306*\"一辆\" + 0.303*\"C++\" + 0.300*\"我\" + 0.300*\"爱\" + 0.205*\"和\" + 0.205*\"Python\"\n",
      "25-03-2016 18:37:21 - lsimodel - gensim.models.lsimodel - INFO - topic #1(1.038): 0.462*\"的士\" + 0.320*\"黑色\" + -0.268*\"我\" + -0.268*\"爱\" + -0.236*\"Python\" + -0.236*\"和\" + -0.204*\"的哥\" + -0.204*\"的\" + -0.204*\"主席\" + -0.204*\"那个\"\n",
      "25-03-2016 18:37:21 - lsimodel - gensim.models.lsimodel - INFO - topic #0(1.077): 0.383*\"的士\" + 0.344*\"黑色\" + 0.306*\"开\" + 0.306*\"他\" + 0.306*\"一辆\" + 0.303*\"C++\" + 0.300*\"我\" + 0.300*\"爱\" + 0.205*\"和\" + 0.205*\"Python\"\n",
      "25-03-2016 18:37:21 - lsimodel - gensim.models.lsimodel - INFO - topic #1(1.038): 0.462*\"的士\" + 0.320*\"黑色\" + -0.268*\"我\" + -0.268*\"爱\" + -0.236*\"Python\" + -0.236*\"和\" + -0.204*\"的哥\" + -0.204*\"的\" + -0.204*\"主席\" + -0.204*\"那个\"\n",
      "25-03-2016 18:37:21 - ldamodel - gensim.models.ldamodel - INFO - using symmetric alpha at 0.5\n",
      "25-03-2016 18:37:21 - ldamodel - gensim.models.ldamodel - INFO - using symmetric eta at 0.5\n",
      "25-03-2016 18:37:21 - ldamodel - gensim.models.ldamodel - INFO - using serial LDA version on this node\n",
      "25-03-2016 18:37:21 - ldamodel - gensim.models.ldamodel - INFO - running online LDA training, 2 topics, 1 passes over the supplied corpus of 4 documents, updating model once every 4 documents, evaluating perplexity every 4 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "25-03-2016 18:37:21 - ldamodel - gensim.models.ldamodel - WARNING - too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
      "25-03-2016 18:37:21 - ldamodel - gensim.models.ldamodel - INFO - -6.994 per-word bound, 127.5 perplexity estimate based on a held-out corpus of 4 documents with 8 words\n",
      "25-03-2016 18:37:21 - ldamodel - gensim.models.ldamodel - INFO - PROGRESS: pass 0, at document #4/4\n",
      "25-03-2016 18:37:21 - ldamodel - gensim.models.ldamodel - INFO - topic #0 (0.500): 0.074*我 + 0.073*爱 + 0.061*和 + 0.059*Python + 0.059*认识 + 0.059*C++ + 0.058*的士 + 0.056*黑色 + 0.053*吗 + 0.053*你\n",
      "25-03-2016 18:37:21 - ldamodel - gensim.models.ldamodel - INFO - topic #1 (0.500): 0.084*的士 + 0.072*黑色 + 0.066*他 + 0.066*开 + 0.064*一辆 + 0.059*C++ + 0.052*Python + 0.051*握手 + 0.051*主席 + 0.051*的\n",
      "25-03-2016 18:37:21 - ldamodel - gensim.models.ldamodel - INFO - topic diff=0.311906, rho=1.000000\n",
      "25-03-2016 18:37:21 - ldamodel - gensim.models.ldamodel - INFO - topic #0 (0.500): 0.074*我 + 0.073*爱 + 0.061*和 + 0.059*Python + 0.059*认识 + 0.059*C++ + 0.058*的士 + 0.056*黑色 + 0.053*吗 + 0.053*你\n",
      "25-03-2016 18:37:21 - ldamodel - gensim.models.ldamodel - INFO - topic #1 (0.500): 0.084*的士 + 0.072*黑色 + 0.066*他 + 0.066*开 + 0.064*一辆 + 0.059*C++ + 0.052*Python + 0.051*握手 + 0.051*主席 + 0.051*的\n",
      "25-03-2016 18:37:21 - docsim - gensim.similarities.docsim - WARNING - scanning corpus to determine the number of features (consider setting `num_features` explicitly)\n",
      "25-03-2016 18:37:21 - docsim - gensim.similarities.docsim - INFO - creating matrix with 4 documents and 2 features\n",
      "25-03-2016 18:37:21 - <ipython-input-43-58ec3b96fc61> - <module> - INFO - sort_sims: [(0, 0.99848497), (3, 0.92187905), (1, 0.018948048), (2, -0.31816125)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['你', '认识', '那个', '和', '主席', '握手', '的', 'Python', '的哥', '吗'], ['他', '开', '一辆', '黑色', 'C++'], ['黑色', '的士'], ['我', '爱', 'Python', '和', 'C++']]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0, 0.99848497), (3, 0.92187905), (1, 0.018948048), (2, -0.31816125)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = [\"你 认识 那个 和 主席 握手 的 Python 的哥 吗\", \"他 开 一辆 黑色 C++\", \"黑色 的士\", \"我 爱 Python 和 C++\"]\n",
    "texts = [[word for word in document.split()] for document in documents]\n",
    "print(texts)\n",
    "query = u'红色 的 Python'\n",
    "gensim4texts(logger, texts, query.lower().split(), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25-03-2016 18:38:29 - <ipython-input-43-58ec3b96fc61> - <module> - INFO - This is gensim4texts\n",
      "25-03-2016 18:38:29 - dictionary - gensim.corpora.dictionary - INFO - adding document #0 to Dictionary(0 unique tokens: [])\n",
      "25-03-2016 18:38:29 - dictionary - gensim.corpora.dictionary - INFO - built Dictionary(16 unique tokens: ['银', '的', '一辆', '火', '运送']...) from 3 documents (total 21 corpus positions)\n",
      "25-03-2016 18:38:29 - tfidfmodel - gensim.models.tfidfmodel - INFO - collecting document frequencies\n",
      "25-03-2016 18:38:29 - tfidfmodel - gensim.models.tfidfmodel - INFO - PROGRESS: processing document #0\n",
      "25-03-2016 18:38:29 - tfidfmodel - gensim.models.tfidfmodel - INFO - calculating IDF weights for 3 documents and 15 features (21 matrix non-zeros)\n",
      "25-03-2016 18:38:29 - lsimodel - gensim.models.lsimodel - INFO - using serial LSI version on this node\n",
      "25-03-2016 18:38:29 - lsimodel - gensim.models.lsimodel - INFO - updating model with new documents\n",
      "25-03-2016 18:38:29 - lsimodel - gensim.models.lsimodel - INFO - preparing a new chunk of documents\n",
      "25-03-2016 18:38:29 - lsimodel - gensim.models.lsimodel - INFO - using 100 extra samples and 2 power iterations\n",
      "25-03-2016 18:38:29 - lsimodel - gensim.models.lsimodel - INFO - 1st phase: constructing (16, 102) action matrix\n",
      "25-03-2016 18:38:29 - lsimodel - gensim.models.lsimodel - INFO - orthonormalizing (16, 102) action matrix\n",
      "25-03-2016 18:38:29 - lsimodel - gensim.models.lsimodel - INFO - 2nd phase: running dense svd on (16, 3) matrix\n",
      "25-03-2016 18:38:29 - lsimodel - gensim.models.lsimodel - INFO - computing the final decomposition\n",
      "25-03-2016 18:38:29 - lsimodel - gensim.models.lsimodel - INFO - keeping 2 factors (discarding 31.810% of energy spectrum)\n",
      "25-03-2016 18:38:29 - lsimodel - gensim.models.lsimodel - INFO - processed documents up to #3\n",
      "25-03-2016 18:38:29 - lsimodel - gensim.models.lsimodel - INFO - topic #0(1.035): -0.399*\"抵达\" + -0.399*\"运送\" + -0.270*\"到达\" + -0.270*\"银色\" + -0.270*\"一辆\" + -0.270*\"银\" + -0.247*\"卡车\" + -0.222*\"黄金\" + -0.204*\"送\" + -0.204*\"受损\"\n",
      "25-03-2016 18:38:29 - lsimodel - gensim.models.lsimodel - INFO - topic #1(0.987): -0.325*\"受损\" + -0.325*\"车\" + -0.325*\"送\" + -0.325*\"在\" + -0.325*\"中\" + -0.325*\"火\" + 0.283*\"一辆\" + 0.283*\"银\" + 0.283*\"银色\" + 0.283*\"到达\"\n",
      "25-03-2016 18:38:29 - lsimodel - gensim.models.lsimodel - INFO - topic #0(1.035): -0.399*\"抵达\" + -0.399*\"运送\" + -0.270*\"到达\" + -0.270*\"银色\" + -0.270*\"一辆\" + -0.270*\"银\" + -0.247*\"卡车\" + -0.222*\"黄金\" + -0.204*\"送\" + -0.204*\"受损\"\n",
      "25-03-2016 18:38:29 - lsimodel - gensim.models.lsimodel - INFO - topic #1(0.987): -0.325*\"受损\" + -0.325*\"车\" + -0.325*\"送\" + -0.325*\"在\" + -0.325*\"中\" + -0.325*\"火\" + 0.283*\"一辆\" + 0.283*\"银\" + 0.283*\"银色\" + 0.283*\"到达\"\n",
      "25-03-2016 18:38:29 - ldamodel - gensim.models.ldamodel - INFO - using symmetric alpha at 0.5\n",
      "25-03-2016 18:38:29 - ldamodel - gensim.models.ldamodel - INFO - using symmetric eta at 0.5\n",
      "25-03-2016 18:38:29 - ldamodel - gensim.models.ldamodel - INFO - using serial LDA version on this node\n",
      "25-03-2016 18:38:29 - ldamodel - gensim.models.ldamodel - INFO - running online LDA training, 2 topics, 1 passes over the supplied corpus of 3 documents, updating model once every 3 documents, evaluating perplexity every 3 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "25-03-2016 18:38:29 - ldamodel - gensim.models.ldamodel - WARNING - too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
      "25-03-2016 18:38:29 - ldamodel - gensim.models.ldamodel - INFO - -7.051 per-word bound, 132.6 perplexity estimate based on a held-out corpus of 3 documents with 6 words\n",
      "25-03-2016 18:38:29 - ldamodel - gensim.models.ldamodel - INFO - PROGRESS: pass 0, at document #3/3\n",
      "25-03-2016 18:38:29 - ldamodel - gensim.models.ldamodel - INFO - topic #0 (0.500): 0.081*运送 + 0.080*抵达 + 0.072*送 + 0.071*在 + 0.070*受损 + 0.069*火 + 0.069*车 + 0.068*中 + 0.063*黄金 + 0.057*卡车\n",
      "25-03-2016 18:38:29 - ldamodel - gensim.models.ldamodel - INFO - topic #1 (0.500): 0.081*到达 + 0.080*银色 + 0.080*银 + 0.080*一辆 + 0.068*卡车 + 0.066*抵达 + 0.064*运送 + 0.060*运 + 0.059*黄金 + 0.055*中\n",
      "25-03-2016 18:38:29 - ldamodel - gensim.models.ldamodel - INFO - topic diff=0.364516, rho=1.000000\n",
      "25-03-2016 18:38:29 - ldamodel - gensim.models.ldamodel - INFO - topic #0 (0.500): 0.081*运送 + 0.080*抵达 + 0.072*送 + 0.071*在 + 0.070*受损 + 0.069*火 + 0.069*车 + 0.068*中 + 0.063*黄金 + 0.057*卡车\n",
      "25-03-2016 18:38:29 - ldamodel - gensim.models.ldamodel - INFO - topic #1 (0.500): 0.081*到达 + 0.080*银色 + 0.080*银 + 0.080*一辆 + 0.068*卡车 + 0.066*抵达 + 0.064*运送 + 0.060*运 + 0.059*黄金 + 0.055*中\n",
      "25-03-2016 18:38:29 - docsim - gensim.similarities.docsim - WARNING - scanning corpus to determine the number of features (consider setting `num_features` explicitly)\n",
      "25-03-2016 18:38:29 - docsim - gensim.similarities.docsim - INFO - creating matrix with 3 documents and 2 features\n",
      "25-03-2016 18:38:29 - <ipython-input-43-58ec3b96fc61> - <module> - INFO - sort_sims: [(2, 0.97292513), (1, 0.9648906), (0, 0.23582897)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['运', '送', '黄金', '的', '车', '在', '火', '中', '受损'], ['一辆', '银色', '的', '运', '银', '卡车', '到达'], ['运送', '黄金', '的', '卡车', '抵达']]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(2, 0.97292513), (1, 0.9648906), (0, 0.23582897)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = [\"运 送 黄金 的 车 在 火 中 受损\", \"一辆 银色 的 运 银 卡车 到达\", \"运送 黄金 的 卡车 抵达\"]\n",
    "texts = [[word for word in document.split()] for document in documents]\n",
    "print(texts)\n",
    "query = u'黄金 银 卡车'\n",
    "gensim4texts(logger, texts, query.split(), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fn = '../cjzpyml/cjzpynlp/data/coursera_corpus'\n",
    "courses = [line.strip() for line in open(fn)]\n",
    "courses_name = [course.split('\\t')[0] for course in courses]\n",
    "logger.debug('courses_name[0:10]=%s', courses_name[0:10])\n",
    "\n",
    "texts = nltkPreProcess4texts(logger, courses) #default stemmer is Lancaster\n",
    "sims = gensim4texts(logger, texts, texts[210], 10)\n",
    "for i, j in enumerate(sims[0:10]):\n",
    "    logger.info('(#,similar_course_name,sims)=(%d,%s,%f)', i, courses_name[j[0]], j[1])\n",
    "\n",
    "texts = nltkPreProcess4texts(logger, courses, stemmer='porter')\n",
    "sims = gensim4texts(logger, texts, texts[210], 10)\n",
    "for i, j in enumerate(sims[0:10]):\n",
    "    logger.info('(#,similar_course_name,sims)=(%d,%s,%f)', i, courses_name[j[0]], j[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fn = '../cjzpyml/cjzpynlp/data/coursera_corpus_c'\n",
    "courses = [line.strip() for line in open(fn)]\n",
    "courses_name = [course.split('\\t')[0] for course in courses]\n",
    "logger.debug('courses_name[0:10]=%s', courses_name[0:10])\n",
    "\n",
    "texts = nltkPreProcess4texts(logger, courses) #default stemmer is Lancaster\n",
    "sims = gensim4texts(logger, texts, texts[210], 10)\n",
    "for i, j in enumerate(sims[0:10]):\n",
    "    logger.info('(#,similar_course_name,sims)=(%d,%s,%f)', i, courses_name[j[0]], j[1])\n",
    "\n",
    "texts = nltkPreProcess4texts(logger, courses, stemmer='porter')\n",
    "sims = gensim4texts(logger, texts, texts[210], 10)\n",
    "for i, j in enumerate(sims[0:10]):\n",
    "    logger.info('(#,similar_course_name,sims)=(%d,%s,%f)', i, courses_name[j[0]], j[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MyCorpus(object):\n",
    "    def __init__(self, logger, fn):\n",
    "        self.fn = fn\n",
    "        self.logger = logger\n",
    "        self.stoplist = set('for a of the and to in'.split())\n",
    "        self.logger.info('I am at class %s', self)\n",
    "    \n",
    "    def getDictionary1(self):\n",
    "        documents = [\"Human machine interface for lab abc computer applications\",\n",
    "                    \"A survey of user opinion of computer system response time\",\n",
    "                    \"The EPS user interface management system\",\n",
    "                    \"System and human system engineering testing of EPS\",\n",
    "                    \"Relation of user perceived response time to error measurement\",\n",
    "                    \"The generation of random binary unordered trees\",\n",
    "                    \"The intersection graph of paths in trees\",\n",
    "                    \"Graph minors IV Widths of trees and well quasi ordering\",\n",
    "                    \"Graph minors A survey\"]\n",
    "# remove common words and tokenize\n",
    "        texts = [[word for word in document.lower().split() if word not in self.stoplist] for document in documents]\n",
    "# remove words that appear only once\n",
    "        all_tokens = sum(texts, [])\n",
    "        tokens_once = set(word for word in set(all_tokens) if all_tokens.count(word) == 1)\n",
    "        texts = [[word for word in text if word not in tokens_once] for text in texts]\n",
    "        self.dictionary1 = gensim.corpora.Dictionary(texts)\n",
    "        self.dictionary1.save('dict_tmp.dict') # store the dictionary, for future reference\n",
    "\n",
    "    def getDictionary(self):\n",
    "# collect statistics about all tokens\n",
    "        self.dictionary = gensim.corpora.Dictionary(line.lower().strip().split() for line in open(self.fn))\n",
    "# remove stop words and words that appear only once\n",
    "        stop_ids = [self.dictionary.token2id[stopword] for stopword in self.stoplist if stopword in self.dictionary.token2id]\n",
    "#        once_ids = [tokenid for tokenid, docfreq in self.dictionary.dfs.iteritems() if docfreq == 1]\n",
    "        once_ids = [tokenid for tokenid, docfreq in self.dictionary.dfs.items() if docfreq == 1]\n",
    "        self.dictionary.filter_tokens(stop_ids + once_ids) # remove stop words and words that appear only once\n",
    "        self.dictionary.compactify() # remove gaps in id sequence after words that were removed\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.getDictionary1()\n",
    "        self.getDictionary()\n",
    "        for line in open(self.fn):\n",
    "# assume there's one document per line, tokens separated by whitespace\n",
    "            yield self.dictionary.doc2bow(line.lower().strip().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25-03-2016 19:06:27 - <ipython-input-71-8e2909e7628b> - <module> - INFO - I am at class <__main__.MyCorpus object at 0x7f98f9b1a1d0>\n",
      "25-03-2016 19:06:27 - dictionary - gensim.corpora.dictionary - INFO - adding document #0 to Dictionary(0 unique tokens: [])\n",
      "25-03-2016 19:06:27 - dictionary - gensim.corpora.dictionary - INFO - built Dictionary(12 unique tokens: ['computer', 'trees', 'eps', 'user', 'graph']...) from 9 documents (total 29 corpus positions)\n",
      "25-03-2016 19:06:27 - utils - gensim.utils - INFO - saving Dictionary object under dict_tmp.dict, separately None\n",
      "25-03-2016 19:06:27 - dictionary - gensim.corpora.dictionary - INFO - adding document #0 to Dictionary(0 unique tokens: [])\n",
      "25-03-2016 19:06:27 - dictionary - gensim.corpora.dictionary - INFO - built Dictionary(58 unique tokens: ['city', 'rock', 'a', 'gas', 'highway,']...) from 4 documents (total 74 corpus positions)\n",
      "25-03-2016 19:06:27 - <ipython-input-72-8713783ad93f> - <module> - INFO - vector=[]\n",
      "25-03-2016 19:06:27 - <ipython-input-72-8713783ad93f> - <module> - INFO - vector=[(0, 1)]\n",
      "25-03-2016 19:06:27 - <ipython-input-72-8713783ad93f> - <module> - INFO - vector=[(0, 1)]\n",
      "25-03-2016 19:06:27 - <ipython-input-72-8713783ad93f> - <module> - INFO - vector=[]\n"
     ]
    }
   ],
   "source": [
    "fn = '../cjzpyml/cjzpynlp/data/mycorpus.txt'\n",
    "corpus_memory_friendly = MyCorpus(logger, fn) # doesn't load the corpus into memory!\n",
    "for vector in corpus_memory_friendly: # load one vector into memory at a time\n",
    "    logger.info('vector=%s', vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25-03-2016 19:06:38 - <ipython-input-71-8e2909e7628b> - <module> - INFO - I am at class <__main__.MyCorpus object at 0x7f98f92ce400>\n",
      "25-03-2016 19:06:38 - dictionary - gensim.corpora.dictionary - INFO - adding document #0 to Dictionary(0 unique tokens: [])\n",
      "25-03-2016 19:06:38 - dictionary - gensim.corpora.dictionary - INFO - built Dictionary(12 unique tokens: ['computer', 'trees', 'eps', 'user', 'graph']...) from 9 documents (total 29 corpus positions)\n",
      "25-03-2016 19:06:38 - utils - gensim.utils - INFO - saving Dictionary object under dict_tmp.dict, separately None\n",
      "25-03-2016 19:06:38 - dictionary - gensim.corpora.dictionary - INFO - adding document #0 to Dictionary(0 unique tokens: [])\n",
      "25-03-2016 19:06:38 - dictionary - gensim.corpora.dictionary - INFO - built Dictionary(50 unique tokens: ['a', 'time', 'trees', 'interface', 'widths']...) from 13 documents (total 94 corpus positions)\n",
      "25-03-2016 19:06:38 - <ipython-input-73-9d3985a4c5a7> - <module> - INFO - vector=[(12, 1), (13, 1)]\n",
      "25-03-2016 19:06:38 - <ipython-input-73-9d3985a4c5a7> - <module> - INFO - vector=[(6, 1), (10, 1), (11, 2)]\n",
      "25-03-2016 19:06:38 - <ipython-input-73-9d3985a4c5a7> - <module> - INFO - vector=[(6, 1), (10, 1), (12, 1), (13, 1)]\n",
      "25-03-2016 19:06:38 - <ipython-input-73-9d3985a4c5a7> - <module> - INFO - vector=[(10, 1), (11, 1), (12, 1)]\n",
      "25-03-2016 19:06:38 - <ipython-input-73-9d3985a4c5a7> - <module> - INFO - vector=[(3, 1), (7, 1), (9, 1)]\n",
      "25-03-2016 19:06:38 - <ipython-input-73-9d3985a4c5a7> - <module> - INFO - vector=[(0, 1), (1, 1), (8, 1), (9, 1), (15, 1), (16, 1)]\n",
      "25-03-2016 19:06:38 - <ipython-input-73-9d3985a4c5a7> - <module> - INFO - vector=[(3, 1), (8, 1), (14, 1), (16, 1)]\n",
      "25-03-2016 19:06:38 - <ipython-input-73-9d3985a4c5a7> - <module> - INFO - vector=[(7, 1), (8, 2), (14, 1)]\n",
      "25-03-2016 19:06:39 - <ipython-input-73-9d3985a4c5a7> - <module> - INFO - vector=[(1, 1), (15, 1), (16, 1)]\n",
      "25-03-2016 19:06:39 - <ipython-input-73-9d3985a4c5a7> - <module> - INFO - vector=[(2, 1)]\n",
      "25-03-2016 19:06:39 - <ipython-input-73-9d3985a4c5a7> - <module> - INFO - vector=[(2, 1), (4, 1)]\n",
      "25-03-2016 19:06:39 - <ipython-input-73-9d3985a4c5a7> - <module> - INFO - vector=[(2, 1), (4, 1), (5, 1)]\n",
      "25-03-2016 19:06:39 - <ipython-input-73-9d3985a4c5a7> - <module> - INFO - vector=[(0, 1), (4, 1), (5, 1)]\n"
     ]
    }
   ],
   "source": [
    "fn = '../cjzpyml/cjzpynlp/data/mycorpus1.txt'\n",
    "corpus_memory_friendly = MyCorpus(logger, fn) # doesn't load the corpus into memory!\n",
    "for vector in corpus_memory_friendly: # load one vector into memory at a time\n",
    "    logger.info('vector=%s', vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
